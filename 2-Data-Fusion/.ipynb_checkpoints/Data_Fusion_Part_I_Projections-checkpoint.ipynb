{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49d91f9b-2dbb-42ca-ae06-e34a85c9295c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ec97f-d142-4b59-9b6f-1c97f19e1865",
   "metadata": {},
   "source": [
    "# Data Fusion in Computer Vision\n",
    "\n",
    "Data fusion in computer vision refers to the technique of integrating data from multiple sensor modalities to enhance tasks such as 3D object detection. There are two principal approaches to data fusion:\n",
    "\n",
    "- **Early Fusion (Data-Level or Feature-Level Fusion)**: This strategy involves merging raw data from different sensors at the beginning of the processing pipeline. The consolidated data is then analyzed using algorithms or Deep Neural Networks (DNNs). Early fusion is particularly effective in capturing the dynamics between different types of sensor data early in the processing stage.\n",
    "\n",
    "- **Late Fusion (Decision-Level Fusion)**: In late fusion, data from each sensor is processed independently using algorithms or DNNs. The individual outputs are then combined to form a final decision. This approach is beneficial when it's important to analyze sensor data separately before integrating their results.\n",
    "\n",
    "### The Logic of Data Fusion\r\n",
    "\r\n",
    "Data fusion is grounded in the principle that different sensors have distinct strengths and can offset each other's weaknesses. For example:\r\n",
    "\r\n",
    "- **LiDAR Sensors**: These are unparalleled in depth perception and distance estimation, outperforming stereo cameras. However, LiDAR can struggle under adverse weather conditions and typically involves higher costs related to resolution. Additionally, LiDAR data is often sparse, offering limited semantic detail about a scene.\r\n",
    "\r\n",
    "- **Cameras**: Cameras excel in object classification and spatial resolution. They provide rich, detailed information about a scene's content, which is essential for understanding and interpreting the environment. Nonetheless, camera performance can also degrade in poor weather.\r\n",
    "\r\n",
    "- **Radar Sensors**: Renowned for their robust velocity estimation, radar sensors are less affected by weather conditions and can provide valuable data, especially in motion analysis.\r\n",
    "\r\n",
    "By integrating data from these varied sources, we can develop more comprehensive and effective AI solutions for computer vision tasks. Each sensor compensates for the limitations of others, enabling a more nuanced and detailed understanding of the environment. This fusion approach is particularly powerful as it leverages the unique capabilities of each sensor type, providing a holistic view that is greater than the sum of its parts.\n",
    "\n",
    "In this tutorial, we will explore data fusion using two specific modalities from the KITTI dataset: the Velodyne HDL-64E LiDAR and the 1.4 Megapixels Point Grey Flea 2 (FL2-14S3C-C) Color Camera.\n",
    "\n",
    "### Sensors in the KITTI Dataset:\n",
    "- `1` Inertial Navigation System (GPS/IMU): OXTS RT 3003\n",
    "- `1` Laserscanner: Velodyne HDL-64E\n",
    "- `2` Grayscale Cameras (1.4 Megapixels): Point Grey Flea 2 (FL2-14S3M-C)\n",
    "- `2` Color Cameras (1.4 Megapixels): Point Grey Flea 2 (FL2-14S3C-C)\n",
    "- `4` Varifocal Lenses (4-8 mm): Edmund Optics NT59-917\n",
    "\n",
    "\n",
    "## Projection from Lidar 3D Space to Image 2D Pixel Space\n",
    "A crucial aspect of early fusion is projecting points from the LiDAR's 3D coordinate frame (`x, y, z`) onto corresponding 2D pixel coordinates (`x, y`) in the image space. This projection is key to integrating spatial and visual data, enhancing the accuracy of object detection systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4682810-dedb-43f5-aba8-e8d8186b4a14",
   "metadata": {},
   "source": [
    "# Kitti Sensor Setup\n",
    "\n",
    "Below is the overall view of how all the sensors where setup in the kitty dataset. \n",
    "\n",
    "![Point Cloud Visualization](../Doc_Images/DATAFUSION_DOC_IMAGES/kitty_setup.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4b6c74-a962-487c-84b4-a599d01144d4",
   "metadata": {},
   "source": [
    "## Projection of LiDAR Data onto Camera Images\r\n",
    "\r\n",
    "Projecting LiDAR data onto camera images necessitates a careful consideration of the differences in their coordinate systems. Let's analyze the coordinate systems in terms of unit vectors for the KITTI dataset senso in the image to the rightrs:\r\n",
    "\r\n",
    "- **Coordinate System of Color Camera `Cam 2`**:\r\n",
    "  - **X-axis Unit Vector**: Points upwards.\r\n",
    "  - **Y-axis Unit Vector**: Points into the page (perpendicular to the image plane).\r\n",
    "  - **Z-axis Unit Vector**: Points to the left (parallel to the image plane).\r\n",
    "\r\n",
    "- **Coordinate System of Velodyne Laserscanner (LiDAR)**:\r\n",
    "  - **X-axis Unit Vector**: Points to the left.\r\n",
    "  - **Y-axis Unit Vector**: Points downwards.\r\n",
    "  - **Z-axis Unit Vector**: Points out of the page (perpendicular to the LiDAR scanning plane).\r\n",
    "\r\n",
    "The physical displacement between these sensors is also a crucial factor, with a separation of approximately 0.27 meters.\r\n",
    "\r\n",
    "### Steps for Accurate 3D to 2D Projection\r\n",
    "To map the 3D LiDAR point cloud data onto the 2D camera image plane effectively, the following transformations are necessary:\r\n",
    "\r\n",
    "1. **Coordinate System Alignment**:\r\n",
    "   - Apply a Rotation Matrix that aligns the LiDAR's unit vectors (`x, y, z`) with those of the camera. This step ensures that the directional orientation of both systems is consistent.\r\n",
    "\r\n",
    "2. **Translation Adjustment**:\r\n",
    "   - Implement a Translation Vector to account for the physical offset between the LiDAR and the camera. This translation corresponds to the real-world displacement and is crucial for accurate overlaying of 3D points onto the 2D image.\r\n",
    "\r\n",
    "By executing these transformations, we can accurately project LiDAR point cloud data onto the camera's image plane, ensuring spatial and directional fidelity between the two sensor modalities.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6054fb3b-7fdd-48b6-9863-bbe963942a5a",
   "metadata": {},
   "source": [
    "## Projecting LiDAR Data onto Camera Images\n",
    "\n",
    "Accurately projecting LiDAR data onto camera images involves more than just determining the rotation and translation matrices between the LiDAR sensor and the camera. It's essential to consider both the intrinsic and extrinsic properties of the camera.\n",
    "\n",
    "### Camera Calibration and Intrinsic Matrix\n",
    "Camera calibration is crucial as it determines the intrinsic properties of the camera. The resulting intrinsic calibration matrix is represented as:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "f_x & 0 & c_x \\\\\n",
    "0 & f_y & c_y \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here, \\(f_x\\) and \\(f_y\\) are the focal lengths along the X and Y axes, respectively, while \\(c_x\\) and \\(c_y\\) represent the optical center of the camera. The camera's ability to project 3D objects onto a 2D image introduces distortions and other parameters that must be accounted for.\n",
    "\n",
    "### Extrinsic Properties\n",
    "The extrinsic properties, such as the camera's position and orientation in the world, are also vital.\n",
    "\n",
    "### Consideration for Vehicle Placement\n",
    "Note: Placing the camera inside a vehicle, such as behind the windshield, introduces additional distortion. The calibration process should consider this factor.\n",
    "\n",
    "### Stereo Vision and Rectification\n",
    "For stereo vision systems, the Stereo Rectification Matrix, a 3x3 matrix, aligns the images from two cameras. If stereo vision is not used, this matrix defaults to the identity matrix.\n",
    "\n",
    "## Projection Formula\n",
    "To derive a 2D pixel coordinate from a 3D LiDAR point, we use the following projection formula:\n",
    "\n",
    "$$\n",
    "Y = P \\cdot R_0 \\cdot [R | t] \\cdot X\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\(Y\\) is the 2D pixel coordinate (Nx2 matrix, N being the number of points).\n",
    "- \\(P\\) is a 3x4 matrix containing the 3x3 intrinsic matrix and a 1x4 extrinsic vector.\n",
    "- \\(R_0\\) is the 3x3 Stereo Rectification Matrix.\n",
    "- \\([R | t]\\) is a 3x4 matrix combining rotation and translation from the LiDAR to the camera coordinate frame.\n",
    "- \\(X\\) is an Nx3 matrix of 3D points from the LiDAR sensor.\n",
    "\n",
    "Enough talk lets do some coding before moving on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec31931e-60c5-463d-a227-4daca2d8749e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../DATA/lidar_data\\\\0000000000.pcd', '../DATA/lidar_data\\\\0000000001.pcd', '../DATA/lidar_data\\\\0000000002.pcd']\n"
     ]
    }
   ],
   "source": [
    "#Lets first load the data\n",
    "lidar_data_path = \"../DATA/lidar_data/*.pcd\"\n",
    "lidar_file_names = sorted(glob.glob(lidar_data_path))\n",
    "print(lidar_file_names[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ce330e-dcf6-443b-b2cb-154e9d9356b2",
   "metadata": {},
   "source": [
    "Let us load the calibration files from the kitty data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1166a559-b8b6-47d2-9ddc-94e65e108c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set path for camera extrinsic and intrinsic vectors and matrices\n",
    "camera_calibration_path = \"../DATA/calibrations/calib_cam_to_cam.txt\"\n",
    "#Set path for rotation and translation matrix from velodyne to camera\n",
    "velodyne_to_camera_calibration_path = \"../DATA/calibrations/calib_velo_to_cam.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5c933f5-89ed-4cf9-bba7-ea7ce8570c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix R0 (3, 3):\n",
      "[[ 9.998817e-01  1.511453e-02 -2.841595e-03]\n",
      " [-1.511724e-02  9.998853e-01 -9.338510e-04]\n",
      " [ 2.827154e-03  9.766976e-04  9.999955e-01]]\n",
      "Matrix P (3, 4):\n",
      "[[7.215377e+02 0.000000e+00 6.095593e+02 4.485728e+01]\n",
      " [0.000000e+00 7.215377e+02 1.728540e+02 2.163791e-01]\n",
      " [0.000000e+00 0.000000e+00 1.000000e+00 2.745884e-03]]\n"
     ]
    }
   ],
   "source": [
    "#Let us review the camera calibration file\n",
    "#We will also use the regex library re for parsing. \n",
    "\"\"\"\n",
    "In the cam_to_cam file we need camera 2 those are the images we have in our DATA folder.\n",
    "These images are not setero. What we need to extract is P_rect_02 and R_rect_02\n",
    "\"\"\"\n",
    "import re\n",
    "P_pattern = \"P_rect_02\"\n",
    "R_0_pattern = \"R_rect_02\"\n",
    "\n",
    "P = None\n",
    "R_0 = None\n",
    "\n",
    "with open(camera_calibration_path) as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            #First see if it is P using re.search\n",
    "            if re.search(P_pattern, line):\n",
    "                #lets split line at spaces and end of line \\n\n",
    "                split_list = re.split(f'[ \\n]', line)\n",
    "                #Remove first element and last to contain just numbers\n",
    "                split_list.pop(0)\n",
    "                split_list.pop(-1)\n",
    "                float_numbers = [float(number) for number in split_list]\n",
    "                #Convert to numpy array and reshape to (3, 4 matrix)\n",
    "                P = np.array(float_numbers).reshape(3,4)\n",
    "                print(f\"Matrix P {P.shape}:\")\n",
    "                print(P)\n",
    "            #Second see if it is R using re.search\n",
    "            elif re.search(R_0_pattern, line):\n",
    "                #lets split line at spaces and end of line \\n\n",
    "                split_list = re.split(f'[ \\n]', line)\n",
    "                #Remove first element and last to contain just numbers\n",
    "                split_list.pop(0)\n",
    "                split_list.pop(-1)\n",
    "                float_numbers = [float(number) for number in split_list]\n",
    "                #Convert to numpy array and reshape to (3, 3 matrix)\n",
    "                R_0 = np.array(float_numbers).reshape(3,3)\n",
    "                print(f\"Matrix R0 {R_0.shape}:\")\n",
    "                print(R_0)\n",
    "        except ValueError as ve:\n",
    "            print(f\"ValueError occurred: {ve}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e7bef-38ae-485e-894c-068537a4767d",
   "metadata": {},
   "source": [
    "Notice how Matrix R0 has values close to 1 on its diagonal and values close to zero everywhere else. This is becuase we are not using stereo imagery, thus the R0 value is the identity matrix or its approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cfe6d3e-b37c-4ebc-a211-6365a8e2322b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix R (3, 3):\n",
      "[[ 7.533745e-03 -9.999714e-01 -6.166020e-04]\n",
      " [ 1.480249e-02  7.280733e-04 -9.998902e-01]\n",
      " [ 9.998621e-01  7.523790e-03  1.480755e-02]]\n",
      "Vector t (3, 1):\n",
      "[[-0.00406977]\n",
      " [-0.07631618]\n",
      " [-0.2717806 ]]\n",
      "Matrix R|t (3, 4):\n",
      "[[ 7.533745e-03 -9.999714e-01 -6.166020e-04 -4.069766e-03]\n",
      " [ 1.480249e-02  7.280733e-04 -9.998902e-01 -7.631618e-02]\n",
      " [ 9.998621e-01  7.523790e-03  1.480755e-02 -2.717806e-01]]\n"
     ]
    }
   ],
   "source": [
    "#Let us now get the velodyne calibration\n",
    "\n",
    "#Define Rotation and Translation matrices to look for\n",
    "R_pattern = \"R:\"\n",
    "t_pattern = \"T:\"\n",
    "\n",
    "R = None\n",
    "t = None\n",
    "with open(velodyne_to_camera_calibration_path) as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            #First see if it is R using re.search\n",
    "            if re.search(R_pattern, line):\n",
    "                #lets split line at spaces and end of line \\n\n",
    "                split_list = re.split(f'[ \\n]', line)\n",
    "                #Remove first element and last to contain just numbers\n",
    "                split_list.pop(0)\n",
    "                split_list.pop(-1)\n",
    "                float_numbers = [float(number) for number in split_list]\n",
    "                #Convert to numpy array and reshape to (3, 4 matrix)\n",
    "                R = np.array(float_numbers).reshape(3,3)\n",
    "                print(f\"Matrix R {R.shape}:\")\n",
    "                print(R)\n",
    "            #Second see if it is T using re.search\n",
    "            elif re.search(t_pattern, line):\n",
    "                #lets split line at spaces and end of line \\n\n",
    "                split_list = re.split(f'[ \\n]', line)\n",
    "                #Remove first element and last to contain just numbers\n",
    "                split_list.pop(0)\n",
    "                split_list.pop(-1)\n",
    "                float_numbers = [float(number) for number in split_list]\n",
    "                #Convert to numpy array and reshape to vector\n",
    "                t = np.array(float_numbers).reshape(-1,1)\n",
    "                print(f\"Vector t {t.shape}:\")\n",
    "                print(t)\n",
    "        except ValueError as ve:\n",
    "            print(f\"ValueError occurred: {ve}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "    #Let us now combine the rotation translation vector to give us R_t 3x4 \n",
    "    R_t = np.concatenate((R, t), axis=1)\n",
    "    print(f\"Matrix R|t {R_t.shape}:\")\n",
    "    print(R_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668aa327-5e7f-4555-82dd-1a6bb3294b8f",
   "metadata": {},
   "source": [
    "## Understanding LiDAR-Camera Calibration\n",
    "\n",
    "![Lidar Camera Calibration](../Doc_Images/DATAFUSION_DOC_IMAGES/lidar_camera_calibration.png)\n",
    "\n",
    "How exactly are these matrices calculated in practice?\n",
    "\n",
    "The process of LiDAR-camera calibration is intricate and goes beyond the scope of this lesson, yet grasping its fundamental concept is essential. Primarily, calibration involves establishing a geometric relationship between the LiDAR and camera coordinate systems. This is commonly achieved through the following methods:\n",
    "\n",
    "### Feature Extraction Using Checkerboards\n",
    "- A checkerboard with known square dimensions is used. \n",
    "- During calibration, an image and a corresponding LiDAR scan are taken to identify the checkerboard corners.\n",
    "- These corners and planes, extracted from both the camera and LiDAR data, help to establish the necessary coordinate alignment.\n",
    "\n",
    "### Alternate Not So Good Method: Correspondence-Based Calibration\n",
    "- This method involves collecting a set of LiDAR points and their corresponding 2D pixel points in the camera image.\n",
    "- The projection from LiDAR to camera can be represented as:\n",
    "\n",
    "  $$\n",
    "  Y = P \\cdot R_0 \\cdot [R | t] \\cdot X\n",
    "  $$\n",
    "\n",
    "  This equation can be simplified to:\n",
    "\n",
    "  $$\n",
    "  Y = A \\cdot X\n",
    "  $$\n",
    "\n",
    "  Where \\( A \\) is the unknown matrix, and \\( X \\) and \\( Y \\) are known. Techniques like least squares or gradient descent can be applied to minimize the error between the actual \\( Y \\) values and the predicted \\( Y' \\) values, thereby estimating \\( A \\).\n",
    "\n",
    "### LiDAR-Camera Calibration in Matlab\n",
    "Matlab offers a robust implementation for LiDAR and camera calibration, providing all necessary matrices for projecting from 3D space to 2D pixel space. More information can be found here: [Matlab LiDAR and Camera Calibration](https://www.mathworks.com/help/lidar/ug/lidar-and-camera-calibration.html).\n",
    "\n",
    "## Time Synchronization\n",
    "Ensuring accurate calibration requires precise time synchronization between image and LiDAR data. Variations in the scanning frequencies of these sensors can lead to alignment issues. Typically, the most effective method for synchronization is to align images and LiDAR scans using the nearest matching timestamps. The higher the scanning frequency of each sensor, the greater the likelihood of achieving precise alignment. While software-based synchronization techniques are commonly used and provide a practical solution, they are not entirely free from errors. In applications where precision is paramount, hardware-based synchronization is advisable for simultaneous data capture and alignment. Fortunately, in our case, we have access to the synchronized dataset from KITI, which simplifies this aspect of the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e51358-bc9c-466c-b4f6-028915618821",
   "metadata": {},
   "source": [
    "## Homogeneous Coordinates in Matrix Multiplication\n",
    "\n",
    "You might have noticed an intriguing detail in matrix projection formula. Consider the projection matrix formula:\n",
    "\n",
    "$$\n",
    "Y = P \\cdot R_0 \\cdot [R | t] \\cdot X\n",
    "$$\n",
    "\n",
    "Breaking it down, we have:\n",
    "\n",
    "$$\n",
    "Y_{N \\times 2} = P_{3x4} \\cdot R_{0_{3x3}} \\cdot [R | t]_{3x4} \\cdot X_{N \\times 3}\n",
    "$$\n",
    "\n",
    "A crucial observation here is the challenge in multiplying `P` and `R0` due to their mismatched dimensions, as the rules of matrix multiplication require the inner dimensions to match (i.e., 3x4 @ 3x3 is not feasible). Also, `R|t` and `X` have mismatched dimensions. To resolve this, we employ homogeneous coordinates, which involve augmenting our matrices to ensure compatible dimensions for multiplication. This conversion is key to project from 3D LiDAR points to 2D image space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37314485-1248-4483-9880-054c5934d9a3",
   "metadata": {},
   "source": [
    "## Homogeneous Coordinates \n",
    "Homogeneous coordinates are a system of coordinates utilized in projective geometry and computer graphics. They extend traditional Cartesian coordinates by adding an additional dimension, enabling the representation of geometric transformations such as translation, rotation, and scaling in a unified manner. This is particularly useful for various matrix operations, like projecting data from LiDAR to camera space, by converting matrices to homogeneous coordinates to ensure compatibility in dimensions for matrix multiplication.\n",
    "\n",
    "### Conversion to Homogeneous Coordinates\n",
    "\n",
    "- **2D Euclidean Point:**\n",
    "  For a point in 2D Cartesian coordinates,\n",
    "  $$\n",
    "  \\begin{bmatrix}\n",
    "  x \\\\\n",
    "  y \n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "  its conversion to homogeneous coordinates involves adding a third dimension. Typically, this third dimension is represented as \\( w \\), which can be any non-zero scalar value. A common choice for  `w` is 1, resulting in:\n",
    "  $$\n",
    "  \\begin{bmatrix}\n",
    "  x \\\\ \n",
    "  y \\\\ \n",
    "  w\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "  where  `w` is typically set to 1 for practical purposes, but it can be any non-zero value.\n",
    "\n",
    "- **3D Euclidean Point:**\n",
    "  Similarly, a point in 3D Cartesian coordinates,\n",
    "  $$\n",
    "  \\begin{bmatrix}\n",
    "  x \\\\ y \\\\ z\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "  is extended in homogeneous coordinates by adding a fourth dimension, usually set to 1:\n",
    "  $$\n",
    "  \\begin{bmatrix}\n",
    "  x \\\\ y \\\\ z \\\\ w\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "  where, as in the 2D case, `w` is typically 1 but can be any non-zero scalar.\n",
    "\n",
    "### From Euclidean to Homogeneous Matrix Representation\n",
    "\n",
    "- **3D Points Matrix (Euclidean Space):**\n",
    "  For our lidar matrix of `N` 3D points in Euclidean space,\n",
    "  $$\n",
    "  \\begin{bmatrix}\n",
    "  x_1 & y_1 & z_1 \\\\\n",
    "  x_2 & y_2 & z_2 \\\\\n",
    "  \\vdots & \\vdots & \\vdots \\\\\n",
    "  x_N & y_N & z_N\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "  the homogeneous representation extends each point with an additional dimension where `w` is set to 1:\n",
    "  $$\n",
    "  \\begin{bmatrix}\n",
    "  x_1 & y_1 & z_1 & 1\\\\\n",
    "  x_2 & y_2 & z_2 & 1\\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "  x_N & y_N & z_N & 1\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "### Rotation Matrix in Homogeneous Coordinates\n",
    "\n",
    "- **3x3 Rotation Matrix (Euclidean Coordinates):**\n",
    "  The standard 3x3 rotation matrix in Euclidean coordinates,\n",
    "  $$\n",
    "  \\begin{pmatrix}\n",
    "  \\cos\\beta\\cos\\gamma & -\\cos\\beta\\sin\\gamma & \\sin\\beta \\\\\n",
    "  \\cos\\alpha\\sin\\gamma + \\cos\\gamma\\sin\\alpha\\sin\\beta & \\cos\\alpha\\cos\\gamma - \\sin\\alpha\\sin\\beta\\sin\\gamma & -\\cos\\beta\\sin\\alpha \\\\\n",
    "  \\sin\\alpha\\sin\\gamma - \\cos\\alpha\\cos\\gamma\\sin\\beta & \\cos\\gamma\\sin\\alpha + \\cos\\alpha\\sin\\beta\\sin\\gamma & \\cos\\alpha\\cos\\beta\n",
    "  \\end{pmatrix}\n",
    "  $$\n",
    "  is expanded in homogeneous coordinates to incorporate the possibility of translations, resulting in:\n",
    "  $$\n",
    "  \\begin{pmatrix}\n",
    "  \\cos\\beta\\cos\\gamma & -\\cos\\beta\\sin\\gamma & \\sin\\beta & 0\\\\\n",
    "  \\cos\\alpha\\sin\\gamma + \\cos\\gamma\\sin\\alpha\\sin\\beta & \\cos\\alpha\\cos\\gamma - \\sin\\alpha\\sin\\beta\\sin\\gamma & -\\cos\\beta\\sin\\alpha & 0 \\\\\n",
    "  \\sin\\alpha\\sin\\gamma - \\cos\\alpha\\cos\\gamma\\sin\\beta & \\cos\\gamma\\sin\\alpha + \\cos\\alpha\\sin\\beta\\sin\\gamma & \\cos\\alpha\\cos\\beta & 0 \\\\\n",
    "  0 & 0 & 0 & 1\n",
    "  \\end{pmatrix}\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8e9549-c1dd-4f8d-8873-47f597ebc15d",
   "metadata": {},
   "source": [
    "## Converting LiDAR and Camera Calibration Matrices to Homogeneous Coordinates\n",
    "\n",
    "Consider the following matrix equation representing a transformation involving LiDAR and camera calibration:\n",
    "$$\n",
    "Y_{N \\times 2} = P_{3 \\times 4} \\cdot R_{0_{3 \\times 3}} \\cdot [R | t]_{3 \\times 4} \\cdot X_{N \\times 3}\n",
    "$$\n",
    "\n",
    "To adapt this equation for homogeneous coordinates, we modify the dimensions of the involved matrices:\n",
    "$$\n",
    "Y = P_{3 \\times 4} \\cdot R_{0_{4 \\times 3}} \\cdot [R | t]_{3 \\times 4} \\cdot X_{N \\times 4}\n",
    "$$\n",
    "\n",
    "However, it's necessary to transpose the matrix \\( X \\) to match the dimensions for multiplication:\n",
    "$$\n",
    "Y = P_{3 \\times 4} \\cdot R_{0_{4 \\times 3}} \\cdot [R | t]_{3 \\times 4} \\cdot X_{4 \\times N}\n",
    "$$\n",
    "\n",
    "Proceeding with the matrix multiplication, we obtain:\n",
    "$$\n",
    "A_{3 \\times 3} = P_{3 \\times 4} \\cdot R_{0_{4 \\times 3}}\n",
    "$$\n",
    "\n",
    "Then, we multiply by the rotation and translation matrix:\n",
    "$$\n",
    "B_{3 \\times 4} = A_{3 \\times 3} \\cdot [R | t]_{3 \\times 4}\n",
    "$$\n",
    "\n",
    "Finally, we arrive at the transformed matrix:\n",
    "$$\n",
    "Y_{3 \\times N} =  B_{3 \\times 4} \\cdot X_{4 \\times N}\n",
    "$$\n",
    "\n",
    "## Converting Back to Euclidean Coordinates\n",
    "\n",
    "The matrix \\( Y \\) is currently \\( 3 \\times N \\) in dimensions, indicating that it is a 2D representation in homogeneous coordinates. To convert \\( Y \\) back to a Euclidean 2D representation, we normalize each point by its third coordinate. This conversion is illustrated as follows:\n",
    "\n",
    "**Homogeneous Coordinates**\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x \\\\ y \\\\ w\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "**Euclidean Coordinates**\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x/w \\\\ y/w\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Alright let us go step by step on converting each matrix to its homogenous representation to carry out the projection! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d9f6b-bcf2-41b7-a3dc-bfd1017a92bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
